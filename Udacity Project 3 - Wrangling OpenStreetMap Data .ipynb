{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Introduction\n",
    "\n",
    "\n",
    "For this project, I have downloaded sample size of Phoenix metropolitan area, predominantly Tempe, AZ as I am doing my Masters in CS at Arizona State University. Selecting Tempe city will help me to validate the data using my personal knowledge of the area. Here's the link to the Overpass API for downloading the OSM of the region.\n",
    "\n",
    "<center> http://overpass-api.de/api/map?bbox=-112.117,33.3394,-111.865,33.4727 </center>\n",
    "\n",
    "Below is the map screenshot of OSM :\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/parthoiiitm/Data-Wrangling-with-OpenStreetMap/master/tempe_screenshot.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "# Data auditing, cleaning, and problems encountered in map\n",
    "\n",
    "First, we want to figure out what kind of elements are present in OSM file, and which are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "OSM_FILE = \"tempeaz.osm\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 24852,\n",
      " 'meta': 1,\n",
      " 'nd': 324070,\n",
      " 'node': 261860,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 549,\n",
      " 'tag': 263230,\n",
      " 'way': 39135}\n"
     ]
    }
   ],
   "source": [
    "def count_tags(filename):\n",
    "    # YOUR CODE HERE\n",
    "    dict_ = {}\n",
    "    for event,elem in ET.iterparse(filename):\n",
    "        if elem.tag not in dict_:\n",
    "            dict_[elem.tag] = 1\n",
    "        else:\n",
    "            dict_[elem.tag] += 1\n",
    "    return dict_\n",
    "\n",
    "\n",
    "tags = count_tags(OSM_FILE)\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the major elements are member, nd, node, relation, tag and way. We will audit these elements, clean them and store them in JSON format in order to be stored in MongoDB database.\n",
    "\n",
    "As the actual file is > 65 MB,  it will take time to manipulate the actual file. So, we will create a sample file called sample.osm, in which we will test our shape element function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to check whether \"k\" value for each \"< tag >\" has any issue or not. To see this, we divided the key type in four categories:\n",
    "\n",
    "* \"lower\", for tags that contain only lowercase letters and are valid \n",
    "* \"lower_colon\", for otherwise valid tags with a colon in their names <br>\n",
    "* \"problemchars\", for tags with problematic characters, and<br>\n",
    "* \"other\", for other tags that do not fall into the other three categories.<br>\n",
    "\n",
    "We check this using the regex expressions and write in a separate file. Then we write the unique key tags belonging to each category in the respective file for later observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 163667, 'lower_colon': 95639, 'other': 3924, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regular expression for each category\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "lo = set()\n",
    "lo_co = set()\n",
    "pro_co = set()\n",
    "oth = set()\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        # YOUR CODE HERE\n",
    "        low = lower.search(element.attrib['k'])\n",
    "        low_col = lower_colon.search(element.attrib['k'])\n",
    "        prob = problemchars.search(element.attrib['k'])\n",
    "        if low:\n",
    "            keys[\"lower\"] += 1\n",
    "            lo.add(element.attrib['k']) \n",
    "        elif low_col:\n",
    "            keys[\"lower_colon\"] +=1\n",
    "            lo_co.add(element.attrib['k']) \n",
    "        elif prob:\n",
    "            keys[\"problemchars\"] +=1\n",
    "            pro_co.add(element.attrib['k']) \n",
    "        else:\n",
    "            keys[\"other\"] +=1\n",
    "            oth.add(element.attrib['k'])\n",
    "        \n",
    "    return keys\n",
    "\n",
    "def write_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        for x in data:\n",
    "            f.write(x + \"\\n\")\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    \n",
    "    write_data(lo, 'lower.txt')\n",
    "    write_data(lo_co, 'lower_colon.txt')\n",
    "    write_data(pro_co, 'problem_chars.txt')\n",
    "    write_data(oth, 'other.txt')\n",
    "    return keys\n",
    "\n",
    "\n",
    "keys = process_map(OSM_FILE)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the other.txt, I found out that there are several keys which are in uppercase (e.g. gnis:County), some consist of numbers (e.g. POP2010), some consist of multiple instances of the same key type (e.g. tiger:name_base_1, tiger:name_base_2) and some keys have more than two colon separators (service : bicycle : screwdriver).\n",
    "\n",
    "We will lowercase all the key tags in shape element for uniformity. For keys with >1 colon separator, we normalized it to two colon separators, as they aren't very frequent keys in usage. For keys with one column separator which are infrequent, we replace it with a single key. We stored it in the dictionary called **replace**, with dictionary value as the key replacing the existing one.\n",
    "\n",
    "replace = {\n",
    "    \n",
    "    'destination:ref:to':'destination:ref_to',\n",
    "    'generator:output:electricity':'output_electricity',\n",
    "    'plant:output:electricity':'plant_output_electricity',\n",
    "    'service :bicycle' :'service_bicycle',\n",
    "    'source:hgv:national_network':'hgv:national_network_source',\n",
    "    'turn:lanes':'turn_lanes', \n",
    "    'turn:lanes:backward':'turn_lanes:backward',\n",
    "    'turn:lanes:forward':'turn_lanes:forward',\n",
    "    'turn:lanes:both_ways':'turn_lanes:both_ways', \n",
    "    'wheelchair:description':'wheelchair_description'\n",
    "}\n",
    "\n",
    "For multiple instances of the same key type, we group it under the parent key type and store in a dictionary called **group**. The dictionary value are the parent key in which the values will be grouped.\n",
    "\n",
    "group = {  \n",
    "    \n",
    "    'name:vi':'name:vi',\n",
    "    'alt_name:vi':'name:vi',\n",
    "    'official_name:vi':'name:vi',\n",
    "    \n",
    "    'name':'name:name',\n",
    "    'name_1':'name:name',\n",
    "    'name_2':'name:name',\n",
    "    'alt_name':'name:name',\n",
    "    \n",
    "    'note':'note:note',\n",
    "    'note_2':'note:note',\n",
    "    'old_ref':'old_ref',\n",
    "    'old_ref2':'old_ref',\n",
    "    \n",
    "    \n",
    "    'tiger:name_base':'tiger:name_base',               \n",
    "    'tiger:name_base_1':'tiger:name_base',\n",
    "    'tiger:name_base_2':'tiger:name_base',\n",
    "    \n",
    "    'tiger:name_direction_prefix':  'tiger:name_direction_prefix',\n",
    "    'tiger:name_direction_prefix_1':  'tiger:name_direction_prefix',\n",
    "    'tiger:name_direction_prefix_2':  'tiger:name_direction_prefix',\n",
    "    \n",
    "    'tiger:name_type' : 'tiger:name_type',\n",
    "    'tiger:name_type_1' : 'tiger:name_type', \n",
    "    'tiger:name_type_2' : 'tiger:name_type',\n",
    "    'tiger:name_type_4' : 'tiger:name_type',\n",
    "    \n",
    "    'tiger:zip_left': 'tiger:zip_left',\n",
    "    'tiger:zip_left_1': 'tiger:zip_left',\n",
    "    'tiger:zip_left_2': 'tiger:zip_left',\n",
    "    'tiger:zip_left_3': 'tiger:zip_left',\n",
    "    'tiger:zip_left_4': 'tiger:zip_left',\n",
    "    \n",
    "    'tiger:zip_right':'tiger:zip_right',\n",
    "    'tiger:zip_right_1':'tiger:zip_right',\n",
    "    'tiger:zip_right_2':'tiger:zip_right',\n",
    "    'tiger:zip_right_3':'tiger:zip_right',\n",
    "    'tiger:zip_right_4':'tiger:zip_right'\n",
    "}\n",
    "\n",
    "Replace and group will be used in shape_element function.\n",
    "\n",
    "There are some keys which are listed in both lower and lower_colon. In cases when they are listed in same node \n",
    "\n",
    "E.g.    \n",
    "     < tag k=\"is_in\" v=\"USA\"/> <br>\n",
    "     < tag k=\"is_in:continent\" v=\"America\"/>\n",
    "\n",
    "It will throw error. So, we want to normalize such single key words from < key > to < key >:< key > (e.g. is_in:is_in) such that it will be easy to nest the sub-keys within a parent key.\n",
    "\n",
    "e.g.    \n",
    "    {\n",
    "    is_in:{\n",
    "            is_in:USA, \n",
    "            continent:America\n",
    "          }\n",
    "    } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['alt_name',\n",
      "     'bridge',\n",
      "     'building',\n",
      "     'capacity',\n",
      "     'communication',\n",
      "     'crossing',\n",
      "     'cycleway',\n",
      "     'destination',\n",
      "     'disused',\n",
      "     'flag',\n",
      "     'golf',\n",
      "     'height',\n",
      "     'hgv',\n",
      "     'internet_access',\n",
      "     'is_in',\n",
      "     'lanes',\n",
      "     'name',\n",
      "     'note',\n",
      "     'old_name',\n",
      "     'oneway',\n",
      "     'operator',\n",
      "     'parking',\n",
      "     'population',\n",
      "     'public_transport',\n",
      "     'ref',\n",
      "     'restriction',\n",
      "     'source',\n",
      "     'toilets',\n",
      "     'traffic_signals',\n",
      "     'wheelchair'])\n"
     ]
    }
   ],
   "source": [
    "lo = set()\n",
    "lo_co = set()\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            spl = l.strip('\\n').split(\":\")\n",
    "            if len(spl) == 2:\n",
    "                #extract the parent key\n",
    "                lo_co.add(spl[0])\n",
    "            else:\n",
    "                lo.add(spl[0])\n",
    "                \n",
    "            \n",
    "\n",
    "read_data('lower.txt')\n",
    "read_data('lower_colon.txt')\n",
    "common = lo.intersection(lo_co)\n",
    "pprint.pprint(common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this potential conflict keys in shape_element. \n",
    "\n",
    "We also need to observe the street names, because in many cases the street names are over-abbreviated, some are abbreviated and some are written in complete form. So, there is a lot of inconsistency in street names. In order to deal with such inconsistencies, we will first audit the street names, and find various variations of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'108': set(['W Elliot Rd #108']),\n",
      " '1400-1532': set(['N. Central Avenue, Suite 1400-1532']),\n",
      " '270': set(['East Washington Street Suite 270']),\n",
      " '900': set(['Rio Salado Parkway #900']),\n",
      " 'Ave': set(['S Central Ave',\n",
      "             'S Farmer Ave',\n",
      "             'South Forest Ave',\n",
      "             'South Longmore Ave',\n",
      "             'Terrace Ave']),\n",
      " 'Blvd.': set(['Apache Blvd.']),\n",
      " 'Circle': set(['South Arizona Mills Circle']),\n",
      " 'Dobson': set(['South Dobson']),\n",
      " 'Longmore': set(['South Longmore']),\n",
      " 'Mall': set(['East Lemon Mall',\n",
      "              'East Orange Mall',\n",
      "              'East Tyler Mall',\n",
      "              'South Cady Mall',\n",
      "              'South Forest Mall']),\n",
      " 'Park': set(['East Gammage Park']),\n",
      " 'Pkwy': set(['East Rio Salado Pkwy']),\n",
      " 'South': set(['East Sky Harbor Circle South']),\n",
      " 'St': set(['W 18th St']),\n",
      " 'Sycamore': set(['North Sycamore']),\n",
      " 'Valencia': set(['South Valencia']),\n",
      " 'Way': set(['West Ikea Way'])}\n"
     ]
    }
   ],
   "source": [
    "#Regular expression to find the street type\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "     # find street type using regex\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        \n",
    "        #if street type not in expected list, we add the new street type and add the name along with it\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "#returns true if key tag is a street name\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "st_types = audit(OSM_FILE)\n",
    "pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above dictionary, we create a **mapping** dictionary to fix the street names. We write a function called update_name to change the street name so that there will be uniformity among all the street names. We use our sample file to test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['West Baseline Road',\n",
      " 'West Baseline Road',\n",
      " 'South Longmore',\n",
      " 'West Portland Street',\n",
      " 'West Portland Street',\n",
      " 'West Roosevelt Street',\n",
      " 'North 4th Avenue',\n",
      " 'North 3rd Avenue',\n",
      " 'North 4th Avenue',\n",
      " 'East Adams Street',\n",
      " 'West McDowell Road',\n",
      " 'North 5th Avenue',\n",
      " 'West Willetta Street',\n",
      " 'West McDowell Road',\n",
      " 'West Lynwood Street',\n",
      " 'West Lynwood Street',\n",
      " 'West Portland Street',\n",
      " 'West Portland Street',\n",
      " 'West Lynwood Street',\n",
      " 'West Willetta Street',\n",
      " 'West Lynwood Street',\n",
      " 'West Portland Street',\n",
      " 'West Portland Street',\n",
      " 'West Culver Street',\n",
      " 'North 5th Avenue',\n",
      " 'North 5th Avenue',\n",
      " 'West Willetta Street',\n",
      " 'West Culver Street',\n",
      " 'North 4th Avenue',\n",
      " 'West Lynwood Street',\n",
      " 'West Lynwood Street',\n",
      " 'West Lynwood Street',\n",
      " 'West Willetta Street',\n",
      " 'North 24th Street',\n",
      " 'East Washington Street',\n",
      " 'East Washington Street',\n",
      " 'East Washington Street',\n",
      " 'East Washington Street',\n",
      " 'North 24th Street',\n",
      " 'East Adams Street',\n",
      " 'North 1st Street',\n",
      " 'East Washington Street',\n",
      " 'East Jackson Street',\n",
      " 'East Madison Street',\n",
      " 'East Madison Street',\n",
      " 'East Jefferson Street',\n",
      " 'North 2nd Avenue',\n",
      " 'West Baseline Road',\n",
      " 'South 17th Drive',\n",
      " 'South 18th Lane',\n",
      " 'South 17th Drive',\n",
      " 'West Willetta Street',\n",
      " 'West Willetta Street',\n",
      " 'West Lynwood Street',\n",
      " 'West Lynwood Street',\n",
      " 'West Coronado Road',\n",
      " 'West Willetta Street',\n",
      " 'West Willetta Street',\n",
      " 'West Willetta Street',\n",
      " 'West Willetta Street',\n",
      " 'West Culver Street',\n",
      " 'West Culver Street',\n",
      " 'West Willetta Street',\n",
      " 'West Willetta Street',\n",
      " 'West Culver Street',\n",
      " 'West Culver Street',\n",
      " 'West Willetta Street',\n",
      " 'West Willetta Street',\n",
      " 'West Culver Street',\n",
      " 'West Willetta Street',\n",
      " 'West Willetta Street',\n",
      " 'West Culver Street',\n",
      " 'West Willetta Street',\n",
      " 'West Moreland Street',\n",
      " 'West Moreland Street',\n",
      " 'West Willetta Street',\n",
      " 'West Moreland Street',\n",
      " 'West Willetta Street',\n",
      " 'West Culver Street',\n",
      " 'South Mill Avenue',\n",
      " 'West 5th Street',\n",
      " 'West Southern Avenue',\n",
      " 'North 1st Avenue',\n",
      " 'South College Avenue',\n",
      " 'East Orange Street',\n",
      " 'East Baseline Road',\n",
      " 'West Baseline Road',\n",
      " 'North Dobson Road',\n",
      " 'West Orion Street',\n",
      " 'South 48th Street',\n",
      " 'Rio Salado Parkway #900',\n",
      " 'North 7th Street',\n",
      " 'East McDowell Road',\n",
      " 'East Roosevelt Street',\n",
      " 'West Van Buren Street',\n",
      " 'West Van Buren Street',\n",
      " 'North 3rd Street',\n",
      " 'South Forest Avenue',\n",
      " 'West Washington Street',\n",
      " 'West Shawnee Drive',\n",
      " 'West 18th Street',\n",
      " 'East Monroe Street',\n",
      " 'West Van Buren Street',\n",
      " 'North 1st Avenue',\n",
      " 'West Van Buren Street',\n",
      " 'North 4th Avenue',\n",
      " 'West Jackson Street',\n",
      " 'North Dobson Road',\n",
      " 'North Dobson Road',\n",
      " 'East Moreland Street',\n",
      " 'South Central Avenue',\n",
      " 'North Central Avenue',\n",
      " 'North 2nd Avenue',\n",
      " 'North Central Avenue',\n",
      " 'North 3rd Avenue',\n",
      " 'North 2nd Avenue',\n",
      " 'North 4th Avenue',\n",
      " 'West Monroe Street',\n",
      " 'South 3rd Avenue',\n",
      " 'West Adams Street',\n",
      " 'West Van Buren Street',\n",
      " 'West Van Buren Street',\n",
      " 'East Sky Harbor Circle South',\n",
      " 'East Culver Street',\n",
      " 'East Willetta Street',\n",
      " 'East Culver Street',\n",
      " 'East Culver Street',\n",
      " 'East Culver Street',\n",
      " 'East Willetta Street',\n",
      " 'East Culver Street',\n",
      " 'East Culver Street',\n",
      " 'East Willetta Street',\n",
      " 'East Willetta Street',\n",
      " 'East Willetta Street',\n",
      " 'East Brill Street',\n",
      " 'East Brill Street',\n",
      " 'East Willetta Street',\n",
      " 'East Brill Street',\n",
      " 'East Brill Street',\n",
      " 'East Roosevelt Street',\n",
      " 'West Lincoln Street',\n",
      " 'North 2nd Avenue',\n",
      " 'North Central Avenue',\n",
      " 'North Central Avenue',\n",
      " 'West Roosevelt Street',\n",
      " 'North Central Avenue',\n",
      " 'West Portland Street',\n",
      " 'West Culver Street',\n",
      " 'East Moreland Street',\n",
      " 'East McDowell Road',\n",
      " 'East Coronado Road',\n",
      " 'North 5th Street',\n",
      " 'West Culver Street',\n",
      " 'Terrace Avenue',\n",
      " 'West Riverview Auto Drive',\n",
      " 'North Dobson Road',\n",
      " 'South Dobson Road',\n",
      " 'East Washington Street',\n",
      " 'East Washington Street',\n",
      " 'East Washington Street',\n",
      " 'South Central Avenue',\n",
      " 'South Central Avenue',\n",
      " 'North 1st Street',\n",
      " 'West Rio Salado Parkway',\n",
      " 'West Baseline Road',\n",
      " 'East 7th Street',\n",
      " 'East Broadway Road',\n",
      " 'North 92nd Street',\n",
      " 'West Baseline Road',\n",
      " 'West Baseline Road',\n",
      " 'West Baseline Road',\n",
      " 'West Broadway Road',\n",
      " 'West La Jolla Drive',\n",
      " 'West Jackson Street',\n",
      " 'West La Jolla Drive',\n",
      " 'West La Jolla Drive',\n",
      " 'North 5th Street',\n",
      " 'West Ruby Drive',\n",
      " 'East McDowell Road',\n",
      " 'East Washington Street',\n",
      " 'East Washington Street',\n",
      " 'North 7th Street',\n",
      " 'West Main Street',\n",
      " 'North 5th Street',\n",
      " 'East Apache Boulevard',\n",
      " 'North 5th Avenue',\n",
      " 'West Southern Avenue',\n",
      " 'West Jefferson Street',\n",
      " 'East Baseline Road',\n",
      " 'West Van Buren Street',\n",
      " 'West Monroe Street',\n",
      " 'East Palm Lane',\n",
      " 'South 32nd Street']\n"
     ]
    }
   ],
   "source": [
    "mapping = {\n",
    "        'Ave':  'Avenue',\n",
    "        'Blvd.': 'Boulevard',\n",
    "        'E':'East',\n",
    "        'E.':'East',\n",
    "        'N.':'North',\n",
    "        'Pkwy':'Parkway',\n",
    "        'Rd':'Road',\n",
    "        'S':'South',\n",
    "        'S.':'South',\n",
    "        'St':'Street',\n",
    "        'W':'West'\n",
    "        }\n",
    "\n",
    "\n",
    "all_street_word = set()\n",
    "name_list = []\n",
    "\n",
    "def audit_street(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag in[\"node\", \"way\", \"relation\"] :\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    better_name = update_name(tag.attrib['v'])\n",
    "                    name_list.append(better_name)\n",
    "                    \n",
    "    osm_file.close()\n",
    "    return name_list\n",
    "\n",
    "def update_name(name):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for n in name.split():\n",
    "        if n in mapping:\n",
    "            name = name.replace(n, mapping[n])\n",
    "    return name\n",
    "\n",
    "\n",
    "\n",
    "st_name = audit_street(SAMPLE_FILE)\n",
    "pprint.pprint(st_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also lot of issues associated with the postal codes. We have written the following code to audit the postal code and find over-abbreviated, under-abbreviated or inconsistent postal codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'addr:postcode': set(['8',\n",
      "                       '85003-1333',\n",
      "                       '85003-1376',\n",
      "                       '85004-1323',\n",
      "                       '85004-1418',\n",
      "                       '85004-1455',\n",
      "                       '85004-1506',\n",
      "                       '85004-1722',\n",
      "                       '85004-1820',\n",
      "                       '85004-1873',\n",
      "                       '85004-4527',\n",
      "                       '85006-3651',\n",
      "                       '85006-3678',\n",
      "                       '85007-1908',\n",
      "                       '85007-1909',\n",
      "                       '85007-2101',\n",
      "                       '85007-2121',\n",
      "                       '85007-2126',\n",
      "                       '85007-2129',\n",
      "                       '85007-2309',\n",
      "                       '85007-2604',\n",
      "                       '85007-2607',\n",
      "                       '85007-2616',\n",
      "                       '85007-3232',\n",
      "                       '85008-4905',\n",
      "                       '85284-1103',\n",
      "                       'AZ 85007',\n",
      "                       'AZ 85008',\n",
      "                       'AZ 85042',\n",
      "                       'AZ 85044',\n",
      "                       'AZ 85202',\n",
      "                       'AZ 85281',\n",
      "                       'AZ 85282',\n",
      "                       'AZ 85283']),\n",
      " 'tiger:zip_left': set(['85017; 85017; 85002',\n",
      "                        '85017;85017;85002',\n",
      "                        '85040:85042',\n",
      "                        '85044:85284',\n",
      "                        '85202:85282',\n",
      "                        '85262; 85254',\n",
      "                        '85262;85254']),\n",
      " 'tiger:zip_right': set(['85009; 85017; 85009',\n",
      "                         '85009;85017;85009',\n",
      "                         '85040:85042',\n",
      "                         '85044:85284',\n",
      "                         '85202:85282',\n",
      "                         '85262; 85254',\n",
      "                         '85262;85254'])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#street_type_re = re.compile(r'\\d{5}([ \\-]\\d{4})?$', re.IGNORECASE)\n",
    "postal_type_re = re.compile(r'^\\d{5}$', re.IGNORECASE)\n",
    "postal_types = defaultdict(set)\n",
    "\n",
    "\n",
    "expected = [ 'tiger:zip_left','tiger:zip_left_1','tiger:zip_left_2',\n",
    "             'tiger:zip_left_3','tiger:zip_left_4', 'tiger:zip_right',\n",
    "             'tiger:zip_right_1','tiger:zip_right_2','tiger:zip_right_3',\n",
    "             'tiger:zip_right_4','addr:postcode']\n",
    "\n",
    "def audit_postal_code(postal_value, elem):\n",
    "    m = postal_type_re.search(postal_value)\n",
    "    if not m:\n",
    "        postal_types[elem.attrib['k']].add(postal_value)\n",
    "\n",
    "def is_postal_code(elem):\n",
    "    return (elem.attrib['k'] in expected)\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag in[\"node\", \"way\", \"relation\"] :\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postal_code(tag):\n",
    "                    audit_postal_code(tag.attrib['v'], tag)\n",
    "    osm_file.close()\n",
    "    return postal_types\n",
    "\n",
    "\n",
    "audit(OSM_FILE)\n",
    "pprint.pprint(dict(postal_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, in tiger:zip_left and tiger:zip_right, the postal codes are separated by semi-colon. We need to reformat that and store it in the list as separate postal code. In addr:postcode, we want to drop all leading state characters (as in \"AZ 85281\") and 4 - digit zip code extensions following a hyphen (as in \"85284-1103\"). Plus, the first result in addr:postcode has postal code '8', it is incorrect and we need to rectify it. To do this, we have a python library called pygeocoder using which, we can get the postal code by just inputting the address.\n",
    "\n",
    "We will do all of this using a function called update_postal_code and to test it on sample file, we will iterate on sample file using audit pin function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85007-2126 85007 1682771572\n",
      "AZ 85008 85008 51990955\n",
      "85004-4527 85004 102348550\n",
      "85007-2616 85007 109835069\n",
      "85007-2604 85007 109835152\n",
      "85006-3678 85006 125115768\n",
      "85004-1418 85004 145385690\n",
      "85004-1873 85004 147227042\n",
      "85017;85017;85002 ['85002', '85017'] 358494996\n",
      "85009;85017;85009 ['85009', '85017'] 358494996\n",
      "85262; 85254 ['85262', '85254'] 402543835\n",
      "85262; 85254 ['85262', '85254'] 402543835\n",
      "85262; 85254 ['85262', '85254'] 436940703\n",
      "85262; 85254 ['85262', '85254'] 436940703\n",
      "85262; 85254 ['85262', '85254'] 436940722\n",
      "85262; 85254 ['85262', '85254'] 436940722\n",
      "85262; 85254 ['85262', '85254'] 436940732\n",
      "85262; 85254 ['85262', '85254'] 436940732\n",
      "85262; 85254 ['85262', '85254'] 436944796\n",
      "85262; 85254 ['85262', '85254'] 436944796\n",
      "85017;85017;85002 ['85002', '85017'] 436970519\n",
      "85009;85017;85009 ['85009', '85017'] 436970519\n",
      "AZ 85042 85042 3547143\n"
     ]
    }
   ],
   "source": [
    "from pygeocoder import Geocoder as gc\n",
    "\n",
    "\n",
    "def is_incorrect_postal_code(postal_value, tag):\n",
    "    if is_postal_code(tag):\n",
    "        m = postal_type_re.search(postal_value)\n",
    "        if not m:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def audit_pin(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag in[\"node\", \"way\", \"relation\"] :\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_incorrect_postal_code(tag.attrib['v'], tag):\n",
    "                    print tag.attrib['v'], update_postal_code(tag.attrib['v'], elem), elem.attrib['id']\n",
    "    osm_file.close()\n",
    "    \n",
    "\n",
    "def update_postal_code(postal_value, element):\n",
    "    if len(postal_value) > 5:\n",
    "        if '-' in postal_value:\n",
    "            return postal_value[:5]\n",
    "        \n",
    "        elif 'AZ' in postal_value:\n",
    "            return postal_value[3:]\n",
    "        \n",
    "        elif ';' in postal_value or ':' in postal_value:\n",
    "            return list(set(x.strip() for x in re.split(';|:',postal_value)))\n",
    "    else:\n",
    "        \n",
    "        # In case the postal code is smaller than 5 digits, we search using \n",
    "        # pygeocoder by inputting address\n",
    "        \n",
    "        home, street, city, state= \"\",\"\",\"\",\"\"\n",
    "        for tag in element.iter('tag'):\n",
    "            if tag.attrib['k'] =='addr:housenumber':\n",
    "                    home = tag.attrib['v'] + \", \"\n",
    "            elif tag.attrib['k'] =='addr:street':\n",
    "                    street = tag.attrib['v'] + \", \"\n",
    "            elif tag.attrib['k'] =='addr:city':\n",
    "                    city = tag.attrib['v'] + \", \"\n",
    "            elif tag.attrib['k'] =='addr:state':\n",
    "                    state = tag.attrib['v']\n",
    "        \n",
    "        address = home+street+city+state\n",
    "        return gc.geocode(address.lower()).postal_code\n",
    "\n",
    "audit_pin(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will wrangle the data and transform the shape of the data into the model we did in quiz. We will use **group** and **replace** dictionary as discussed above, and use a list called **conflict_keys** to store the keys which may cause conflict and modify them.\n",
    "\n",
    "The details of the particular data node creation such as user id , timestamp etc. are grouped under key called **created** .The attributes for latitude and longitude are added to a **pos** array. If the tag have keys separated by ':', I have aggregated related descriptors into single top level keys. \n",
    "\n",
    "E.g.\n",
    "\n",
    "    {\n",
    "    \"id\": \"2406124091\",\n",
    "    \"type: \"node\",\n",
    "    \"visible\":\"true\",\n",
    "    \"created\": {\n",
    "              \"version\":\"2\",\n",
    "              \"changeset\":\"17206049\",\n",
    "              \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
    "              \"user\":\"linuxUser16\",\n",
    "              \"uid\":\"1219059\"\n",
    "            },\n",
    "    \"pos\": [41.9757030, -87.6921867],\n",
    "    \"addr\": {\n",
    "              \"housenumber\": \"5157\",\n",
    "              \"postcode\": \"60625\",\n",
    "              \"street\": \"North Lincoln Ave\"\n",
    "            },\n",
    "    \"amenity\": \"restaurant\",\n",
    "    \"cuisine\": \"mexican\",\n",
    "    \"name\": \"La Cabana De Don Luis\",\n",
    "    \"phone\": \"1 (773)-271-5176\"\n",
    "    }\n",
    "    \n",
    "    :\n",
    "    :\n",
    "\n",
    "This will result in a more organized structure of data.\n",
    "\n",
    "In case of  \"way\" tags, we encounter node references as follows:\n",
    "\n",
    "      <nd ref=\"305896090\"/>\n",
    "      <nd ref=\"1719825889\"/>\n",
    "\n",
    "They are grouped under node_refs as follows:\n",
    "\"node_refs\": [\"305896090\", \"1719825889\"]\n",
    "\n",
    "Member tags come under relations. I observed that in all instances, member elements **type** and **role** are common, only the ref varied. So, I  decided to group the member references in a list according to type and role under member dictionary. Here, inside member dictionary, the key will be the type, and the value will be a dictionary consisting of role and member_refs. \n",
    "\n",
    "E.g. \n",
    "\n",
    "    <member type=\"way\" ref=\"30650245\" role=\"outer\"/>\n",
    "    <member type=\"way\" ref=\"30650242\" role=\"outer\"/>\n",
    "    <member type=\"way\" ref=\"30420603\" role=\"inner\"/>\n",
    "    <member type=\"way\" ref=\"30420613\" role=\"inner\"/>\n",
    "\n",
    "will be turned as\n",
    "\n",
    "    \"member\": {\n",
    "        \"way\": [\n",
    "          {\n",
    "            \"member_refs\": [\n",
    "              \"30650245\",\n",
    "              \"30650242\"\n",
    "            ],\n",
    "            \"role\": \"outer\"\n",
    "          },\n",
    "          {\n",
    "            \"member_refs\": [\n",
    "              \"30420603\",\n",
    "              \"30420613\"\n",
    "            ],\n",
    "            \"role\": \"inner\"\n",
    "          }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dict to rename the single key which can cause conflict while nesting\n",
    "\n",
    "conflict_keys = [\n",
    "                 'building',\n",
    "                 'capacity',\n",
    "                 'crossing',\n",
    "                 'cycleway',\n",
    "                 'destination',\n",
    "                 'height',\n",
    "                 'hgv',\n",
    "                 'iso3166-1',\n",
    "                 'internet_access',\n",
    "                 'is_in',\n",
    "                 'lanes',\n",
    "                 'old_name',\n",
    "                 'oneway',\n",
    "                 'operator',\n",
    "                 'population',\n",
    "                 'ref',\n",
    "                 'source',\n",
    "                 'traffic_signals'\n",
    "                 #'bridge',         # Commented keys don't cause any conflicy, so it's not needed \n",
    "                 #'communication',  # to unnecessarily nest them\n",
    "                 #'disused',\n",
    "                 #'flag',\n",
    "                 #'golf',\n",
    "                 #'name',\n",
    "                 #'note',\n",
    "                 #'parking',\n",
    "                 #'public_transport',\n",
    "                 #'restriction',\n",
    "                 #'toilets',\n",
    "                 #'turn_lanes',\n",
    "                 #'wheelchair'\n",
    "                ]\n",
    "\n",
    "#Dict to rename the key\n",
    "replace = {\n",
    "    \n",
    "    'destination:ref:to':'destination:ref_to',\n",
    "    'generator:output:electricity':'output_electricity',\n",
    "    'plant:output:electricity':'plant_output_electricity',\n",
    "    'service:bicycle':'service_bicycle',\n",
    "    'source:hgv:national_network':'hgv:national_network_source',\n",
    "    'turn:lanes':'turn_lanes',\n",
    "    'turn:lanes:backward':'turn_lanes:backward',\n",
    "    'turn:lanes:forward':'turn_lanes:forward',\n",
    "    'turn:lanes:both_ways':'turn_lanes:both_ways', \n",
    "    'wheelchair:description':'wheelchair_description'\n",
    "}\n",
    "\n",
    "#Dict to group in common element\n",
    "group = {  \n",
    "    \n",
    "    'name:vi':'name:vi',\n",
    "    'alt_name:vi':'name:vi',\n",
    "    'official_name:vi':'name:vi',\n",
    "    \n",
    "    'name':'name:name',\n",
    "    'name_1':'name:name',\n",
    "    'name_2':'name:name',\n",
    "    'alt_name':'name:name',\n",
    "    \n",
    "    'note':'note:note',\n",
    "    'note_2':'note:note',\n",
    "    'old_ref':'old_ref',\n",
    "    'old_ref2':'old_ref',\n",
    "    \n",
    "    \n",
    "    'tiger:name_base':'tiger:name_base',               \n",
    "    'tiger:name_base_1':'tiger:name_base',\n",
    "    'tiger:name_base_2':'tiger:name_base',\n",
    "    'tiger:name_base_3':'tiger:name_base',\n",
    "    'tiger:name_base_4':'tiger:name_base',\n",
    "    \n",
    "    'tiger:name_direction_prefix':  'tiger:name_direction_prefix',\n",
    "    'tiger:name_direction_prefix_1':  'tiger:name_direction_prefix',\n",
    "    'tiger:name_direction_prefix_2':  'tiger:name_direction_prefix',\n",
    "    \n",
    "    'tiger:name_type' : 'tiger:name_type',\n",
    "    'tiger:name_type_1' : 'tiger:name_type', \n",
    "    'tiger:name_type_2' : 'tiger:name_type',\n",
    "    'tiger:name_type_3' : 'tiger:name_type',\n",
    "    'tiger:name_type_4' : 'tiger:name_type',\n",
    "    \n",
    "    'tiger:zip_left': 'tiger:zip_left',\n",
    "    'tiger:zip_left_1': 'tiger:zip_left',\n",
    "    'tiger:zip_left_2': 'tiger:zip_left',\n",
    "    'tiger:zip_left_3': 'tiger:zip_left',\n",
    "    'tiger:zip_left_4': 'tiger:zip_left',\n",
    "    \n",
    "    'tiger:zip_right':'tiger:zip_right',\n",
    "    'tiger:zip_right_1':'tiger:zip_right',\n",
    "    'tiger:zip_right_2':'tiger:zip_right',\n",
    "    'tiger:zip_right_3':'tiger:zip_right',\n",
    "    'tiger:zip_right_4':'tiger:zip_right'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag in ['node','way','relation']:\n",
    "        node['id'] = element.attrib['id']\n",
    "        node['tag_type'] = element.tag\n",
    "        node['created'] = {k: element.attrib[k] for k in CREATED}\n",
    "        if element.tag == 'node':\n",
    "            node['pos'] = [float(element.attrib['lat']), float(element.attrib['lon'])]\n",
    "\n",
    "        for tag in element.iter('tag'):\n",
    "            \n",
    "            #Lowercase all the keys\n",
    "            attribute_string =  tag.attrib['k'].lower()\n",
    "            \n",
    "            #Dictionary key formation to rename the single key which can cause conflict while nesting\n",
    "            if attribute_string in conflict_keys:\n",
    "                if attribute_string not in node:\n",
    "                    node[attribute_string] = {attribute_string : tag.attrib['v']}\n",
    "                    \n",
    "                else:\n",
    "                    node[attribute_string].update({attribute_string : tag.attrib['v']})\n",
    "            \n",
    "        \n",
    "            \n",
    "            # Rename the existing key in dictionary \n",
    "            elif attribute_string in replace:\n",
    "                replaced_key = replace[attribute_string]\n",
    "                replaced_key_list = replaced_key.split(':')\n",
    "                replaced_key_length = len(replaced_key_list)\n",
    "                \n",
    "                # if key is separated by ':'\n",
    "                if replaced_key_length == 2:\n",
    "                    if replaced_key_list[0] not in node:\n",
    "                        node[replaced_key_list[0]] = { replaced_key_list[0] : tag.attrib['v'] }\n",
    "                    else:\n",
    "                        node[replaced_key_list[0]].update({ replaced_key_list[0] : tag.attrib['v'] })\n",
    "                \n",
    "                # if key is a single word\n",
    "                elif replaced_key_list[0] not in node:\n",
    "                        node[replaced_key_list[0]] = tag.attrib['v']\n",
    "                \n",
    "                        \n",
    "            # Group values in a common parent key  \n",
    "            elif attribute_string in group:\n",
    "                group_list = group[attribute_string].split(\":\")\n",
    "                \n",
    "                # if key is separated by ':'\n",
    "                if len(group_list) == 2:\n",
    "                    if group_list[0] not in node:\n",
    "                        node[group_list[0]] = { group_list[1]:[tag.attrib['v']] }\n",
    "                    elif group_list[1] not in node[group_list[0]]:\n",
    "                        node[group_list[0]][group_list[1]] = [tag.attrib['v']] \n",
    "                    else:\n",
    "                        node[group_list[0]][group_list[1]].append(tag.attrib['v'])\n",
    "                    \n",
    "                    \n",
    "                    #correct postal codes for tiger:zip_left or tiger:zip_right\n",
    "                    if audit_postal_code_modified(tag.attrib['v'], tag):  \n",
    "                        node[group_list[0]][ group_list[1] ] = update_postal_code(tag.attrib['v'], element)\n",
    "                        \n",
    "                    \n",
    "                    #group unique elements\n",
    "                    node[group_list[0]][group_list[1]] = list(set(node[group_list[0]][group_list[1]]))\n",
    "\n",
    "                # if key is a single word\n",
    "                else:\n",
    "                    if group_list[0] not in node:\n",
    "                        node[group_list[0]] = [tag.attrib['v']]\n",
    "                    else:\n",
    "                        node[group_list[0]].append(tag.attrib['v'])\n",
    "                    \n",
    "                    #group unique elements\n",
    "                    node[group_list[0]] = list(set(node[group_list[0]]))\n",
    "            \n",
    "            \n",
    "            #Handle all other cases\n",
    "            else:\n",
    "                other_list = attribute_string.split(\":\")\n",
    "                if len(other_list) == 2:\n",
    "                    if other_list[0] not in node:\n",
    "                        node[other_list[0]] = {other_list[1]:tag.attrib['v']}\n",
    "                    else:\n",
    "                        node[other_list[0]].update({other_list[1]:tag.attrib['v']})\n",
    "                    \n",
    "                    # Correct the street names \n",
    "                    if attribute_string == \"addr:street\" :\n",
    "                        node[other_list[0]][ other_list[1] ] = update_name(tag.attrib['v'])\n",
    "                    \n",
    "                    # Correct the postal code in case of addr:postcode\n",
    "                    if audit_postal_code_modified(tag.attrib['v'], tag):\n",
    "                            node[other_list[0]][ other_list[1] ] = update_postal_code(tag.attrib['v'], element)\n",
    "                \n",
    "                else:\n",
    "                    node[other_list[0]] = tag.attrib['v']\n",
    "        \n",
    "        # Group all node reference ids belonging to a node in a list\n",
    "        if element.tag == 'way':\n",
    "            node[\"node_refs\"] = []\n",
    "            for tag in element.iter('nd'):\n",
    "                node['node_refs'].append(tag.attrib['ref'])\n",
    "        \n",
    "        # Grouping of member tag in the node as described above\n",
    "        if element.tag == 'relation':\n",
    "            node['member'] = {}\n",
    "            \n",
    "            for tag in element.iter('member'):\n",
    "                # In case the nmember dictionary doesn't exist, we create one\n",
    "                if tag.attrib['type'] not in node['member']:\n",
    "                    # in case the role is blank, we will group under 'type' key \n",
    "                    # of member with role as 'NA', meaning Not Available\n",
    "                    if tag.attrib['role']==\"\":\n",
    "                        node['member'].update({tag.attrib['type']:[{'role':'NA',\n",
    "                                                                   'member_refs':[tag.attrib['ref']]}]})\n",
    "                    else:\n",
    "                        #Otherwise we group with respective role, and also group the member references in list\n",
    "                        node['member'].update({tag.attrib['type']:[{'role': tag.attrib['role'], \n",
    "                                                               'member_refs':[tag.attrib['ref']]}]})\n",
    "                else:\n",
    "                    if tag.attrib['role']==\"\":\n",
    "                        for i,item in enumerate(node['member'][tag.attrib['type']]):\n",
    "                            if item['role']=='NA':\n",
    "                                item['member_refs'].append(tag.attrib['ref'])\n",
    "                                break\n",
    "                    else:\n",
    "                        # We need to push the member reference in the appropriate dictionary among\n",
    "                        # the list of dictionaries, according to the role\n",
    "                        if not any(d.get('role', None) == tag.attrib['role'] for d in node['member'][tag.attrib['type']]):\n",
    "                            node['member'][tag.attrib['type']].append({'role':tag.attrib['role'],\n",
    "                                                                       'member_refs':[tag.attrib['ref']]})\n",
    "                        else:\n",
    "                            for i,item in enumerate(node['member'][tag.attrib['type']]):\n",
    "                                if item['role']==tag.attrib['role']:\n",
    "                                    item['member_refs'].append(tag.attrib['ref'])\n",
    "                                    break\n",
    "\n",
    "\n",
    "        #remove keys with empty values\n",
    "        node = {k: v for k, v in node.items() if v}\n",
    "        \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "\n",
    "#data = process_map('sample.osm', False)\n",
    "data = process_map('tempe.osm', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Overview of the Data\n",
    "\n",
    "After converting the OSM data in JSON format, we want to store the data in our MongoDB database, so that we can perform some queries and get an insight into data. I have imported the JSON data in MongoDB in database called **tempeosm**. It can be seen as follows:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/parthoiiitm/Data-Wrangling-with-OpenStreetMap/master/db_import.png\" width=\"700\" height=\"700\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "db = get_db('tempeosm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have some basic statistics about the dataset and the MongoDB queries used to gather them.\n",
    " \n",
    "### File Size\n",
    "\n",
    "* tempe.osm ......... 66 MB\n",
    "* tempe.osm.json .... 69 MB\n",
    "\n",
    "### Number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300751"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_length = db.tempeosm.find().count()\n",
    "doc_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261354"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.tempeosm.find({'tag_type':'node'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38851"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.tempeosm.find({'tag_type':'way'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.tempeosm.find({'tag_type':'relation'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367\n"
     ]
    }
   ],
   "source": [
    "print len(db.tempeosm.distinct(\"created.user\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top contributing user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr Kludge 169093\n"
     ]
    }
   ],
   "source": [
    "def make_contributor_pipeline():\n",
    "    \n",
    "    pipeline = [{\"$group\":{\"_id\":\"$created.user\", \n",
    "                            \"count\":{\"$sum\":1}}}, \n",
    "                       {\"$sort\":{\"count\":-1}}, \n",
    "                       {\"$limit\":1}]\n",
    "    return pipeline\n",
    "\n",
    "def top_results(db, pipeline):\n",
    "    return [top for top in db.tempeosm.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "pipeline = make_contributor_pipeline()\n",
    "result = top_contributor(db, pipeline)\n",
    "for i in result:\n",
    "    print i['_id'], i['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 major postal code in Phoenix Metropolitan Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85006 207\n",
      "85003 112\n",
      "85004 87\n",
      "85281 85\n",
      "85201 50\n"
     ]
    }
   ],
   "source": [
    "def make_postal_code_pipeline():\n",
    "    \n",
    "    pipeline = [{\"$match\":{\"addr.postcode\":{\"$exists\":1}}}, \n",
    "                       {\"$group\":{\"_id\":\"$addr.postcode\",\n",
    "                                  \"count\":{\"$sum\":1}}}, \n",
    "                       {\"$sort\":{\"count\":-1}},\n",
    "                       {\"$limit\":5}]\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "pipeline = make_postal_code_pipeline()\n",
    "result = top_results(db, pipeline)\n",
    "\n",
    "for i in result:\n",
    "    print i['_id'], i['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional ideas about the dataset\n",
    "\n",
    " Here are some user percentage statistics [1] :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 169093, u'percentage': 56.22358695399184, u'_id': u'Dr Kludge'}\n",
      "{u'count': 31392, u'percentage': 10.437870530771303, u'_id': u'TheDutchMan13'}\n",
      "{u'count': 11832, u'percentage': 3.9341515073931594, u'_id': u'jfuredy'}\n",
      "{u'count': 7695, u'percentage': 2.558594983890328, u'_id': u'dwh1985'}\n",
      "{u'count': 6255, u'percentage': 2.0797935833962318, u'_id': u'woodpeck_fixbot'}\n"
     ]
    }
   ],
   "source": [
    "def make_topuser_pipeline():\n",
    "    \n",
    "    pipeline = [ \n",
    "                    {\"$group\":{\"_id\":\"$created.user\",\n",
    "                               \"count\":{\"$sum\":1}}},\n",
    "                    {\"$project\":{\"count\":1,\n",
    "                                 \"percentage\":{\"$multiply\":[{\"$divide\":[100,doc_length]},\"$count\"]}}\n",
    "                    },\n",
    "                    {\"$sort\":{\"count\":-1}}, \n",
    "                    {\"$limit\":5}\n",
    "                ]\n",
    "        \n",
    "    return pipeline\n",
    "\n",
    "pipeline = make_topuser_pipeline()\n",
    "result = top_results(db, pipeline)\n",
    "\n",
    "for i in result:\n",
    "    print  i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top user contribution percentage (\"Dr Kludge\") - 56.22%\n",
    "\n",
    "Top 5 Users' contributions:\n",
    "\n",
    "    * Dr Kludge - 169093  \n",
    "        ~56.22% \n",
    "        \n",
    "    * TheDutchMan13 - 31392 \n",
    "        ~10.44% \n",
    "\n",
    "    * jfuredy - 11832 \n",
    "        ~3.93% \n",
    "\n",
    "    * dwh1985 - 7695 \n",
    "        ~2.56% \n",
    "\n",
    "    * woodpeck_fixbot - 6255 \n",
    "        ~2.08% \n",
    "    \n",
    "Combined Top 5 users contribution percentage : 75.23%\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Let's explore some more aspects of the data:**\n",
    "\n",
    "As I go to ASU daily by bike, the following is one of the most important query of my interest.\n",
    "\n",
    "### Top 3 kinds of bicycle roads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 367, u'_id': u'asphalt'}\n",
      "{u'count': 15, u'_id': u'concrete'}\n",
      "{u'count': 12, u'_id': u'paved'}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$match\":{\"bicycle\":{\"$exists\":1}, \"bicycle\": \"designated\"}},\n",
    "                                {\"$group\":{\"_id\":\"$surface\", \"count\":{\"$sum\":1}}},\n",
    "                                {\"$sort\":{\"count\":-1}}, {\"$limit\":3}]\n",
    "\n",
    "result = top_results(db, pipeline)\n",
    "for i in result:\n",
    "    print  i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Tempe's demography is mostly consisted of students, let's see what are the top amenities in Tempe. [1]\n",
    "\n",
    "### Top 5 amenities in Tempe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parking\n",
      "restaurant\n",
      "fast_food\n",
      "school\n",
      "place_of_worship\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}}}, \n",
    "                {\"$group\":{\"_id\":\"$amenity\",\n",
    "                            \"count\":{\"$sum\":1}}},\n",
    "                {\"$sort\":{\"count\":-1}}, \n",
    "                {\"$limit\":5}]\n",
    "\n",
    "\n",
    "result = top_results(db, pipeline)\n",
    "\n",
    "for i in result:\n",
    "    print i['_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, we see that the above amenities are in conform with basic needs of student life. \n",
    "\n",
    "We students love to eat a lot of fast food as well. Let's see the following query. [2]\n",
    "\n",
    "### 3 most popular fast foods in Tempe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burger 31\n",
      "sandwich 10\n",
      "mexican 9\n"
     ]
    }
   ],
   "source": [
    "def make_cuisine_pipeline():\n",
    "    \n",
    "    pipeline = [{\"$match\":{\"amenity\":\"fast_food\", \n",
    "                            \"cuisine\":{\"$exists\":1}}}, \n",
    "                       {\"$group\":{\"_id\":\"$cuisine\", \n",
    "                            \"count\":{\"$sum\":1}}}, \n",
    "                       {\"$sort\":{\"count\":-1}}, \n",
    "                       {\"$limit\":3}]\n",
    "    return pipeline\n",
    "\n",
    "pipeline = make_cuisine_pipeline()\n",
    "result = top_results(db, pipeline)\n",
    "\n",
    "for i in result:\n",
    "    print i['_id'], i['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I have interestingly noticed that bots are one of the major contributors of geotagging data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Categorization of data has only been done on the subset of Phoenix data, and the data auditing functions and dictionaries (e.g., group, replace) made on the basis of subset data might not be enough to correctly categorize and remove all kinds of problem from the original Phoenix data (~600 MB).\n",
    "\n",
    "\n",
    "\n",
    "* A good amount of positional data is available in the OSM file. Using more features of pygeocoder and other libraries, I guess we can clean the data even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] OpenStreetMap Sample Project Data Wrangling with MongoDB by Matthew Banbury\n",
    "    https://docs.google.com/document/d/1F0Vs14oNEs2idFJR3C_OPxwS6L0HPliOii-QpbmrMo4/pub\n",
    "    \n",
    "[2] Exploratory Analysis of Open Street Map Data by Miadad Rashid                \n",
    "    http://www.slideshare.net/MiadadRashid/project-48464910?ref=https://www.linkedin.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX : Python code for Case Study quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quiz 1: Iterative Parsing\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task is to use the iterative parsing to process the map file and\n",
    "find out not only what tags are there, but also how many, to get the\n",
    "feeling on how much of which data you can expect to have in the map.\n",
    "Fill out the count_tags function. It should return a dictionary with the \n",
    "tag name as the key and number of times this tag can be encountered in \n",
    "the map as value.\n",
    "\n",
    "Note that your code will be tested with a different data file than the 'example.osm'\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "def count_tags(filename):\n",
    "    # YOUR CODE HERE\n",
    "    dict_ = {}\n",
    "    for event,elem in ET.iterparse(filename):\n",
    "        if elem.tag not in dict_:\n",
    "            dict_[elem.tag] = 1\n",
    "        else:\n",
    "            dict_[elem.tag] += 1\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    tags = count_tags('example.osm')\n",
    "    pprint.pprint(tags)\n",
    "    assert tags == {'bounds': 1,\n",
    "                     'member': 3,\n",
    "                     'nd': 4,\n",
    "                     'node': 20,\n",
    "                     'osm': 1,\n",
    "                     'relation': 1,\n",
    "                     'tag': 7,\n",
    "                     'way': 1}\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 2: Data Model\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/parthoiiitm/Data-Wrangling-with-OpenStreetMap/master/quiz.png\" width=\"500\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#QUIZ 3: Tag Types\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\"\"\"\n",
    "Your task is to explore the data a bit more.\n",
    "Before you process the data and add it into your database, you should check the\n",
    "\"k\" value for each \"<tag>\" and see if there are any potential problems.\n",
    "\n",
    "We have provided you with 3 regular expressions to check for certain patterns\n",
    "in the tags. As we saw in the quiz earlier, we would like to change the data\n",
    "model and expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "{\"address\": {\"street\": \"Some value\"}}\n",
    "So, we have to see if we have such tags, and if we have any tags with\n",
    "problematic characters.\n",
    "\n",
    "Please complete the function 'key_type', such that we have a count of each of\n",
    "four tag categories in a dictionary:\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories.\n",
    "See the 'process_map' and 'test' functions for examples of the expected format.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        # YOUR CODE HERE\n",
    "        low = lower.search(element.attrib['k'])\n",
    "        low_col = lower_colon.search(element.attrib['k'])\n",
    "        prob = problemchars.search(element.attrib['k'])\n",
    "        if low:\n",
    "            keys[\"lower\"] += 1\n",
    "        elif low_col:\n",
    "            keys[\"lower_colon\"] +=1\n",
    "        elif prob:\n",
    "            keys[\"problemchars\"] +=1\n",
    "        else:\n",
    "            keys[\"other\"] +=1\n",
    "        \n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    # You can use another testfile 'map.osm' to look at your solution\n",
    "    # Note that the assertion below will be incorrect then.\n",
    "    # Note as well that the test function here is only used in the Test Run;\n",
    "    # when you submit, your code will be checked against a different dataset.\n",
    "    keys = process_map('example.osm')\n",
    "    pprint.pprint(keys)\n",
    "    assert keys == {'lower': 5, 'lower_colon': 0, 'other': 1, 'problemchars': 1}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#QUIZ 4: Exploring Users\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\"\"\"\n",
    "Your task is to explore the data a bit more.\n",
    "The first task is a fun one - find out how many unique users\n",
    "have contributed to the map in this particular area!\n",
    "\n",
    "The function process_map should return a set of unique user IDs (\"uid\")\n",
    "\"\"\"\n",
    "\n",
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if 'uid' in element.attrib:\n",
    "            users.add(element.attrib['uid'])\n",
    "\n",
    "    return users\n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    users = process_map('example.osm')\n",
    "    pprint.pprint(users)\n",
    "    assert len(users) == 6\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QUIZ 5: Improving Street Names\n",
    "\n",
    "\"\"\"\n",
    "Your task in this exercise has two steps:\n",
    "\n",
    "- audit the OSMFILE and change the variable 'mapping' to reflect the changes needed to fix \n",
    "    the unexpected street types to the appropriate ones in the expected list.\n",
    "    You have to add mappings only for the actual problems you find in this OSMFILE,\n",
    "    not a generalized solution, since that may and will depend on the particular area you are auditing.\n",
    "- write the update_name function, to actually fix the street name.\n",
    "    The function takes a string with street name as an argument and should return the fixed name\n",
    "    We have provided a simple test so that you see what exactly is expected\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"example.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\":\"Avenue\",\n",
    "            \"Rd.\":\"Road\"\n",
    "            }\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    last_word = name.split()[-1]\n",
    "    return name.replace(last_word, mapping[last_word])\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    assert len(st_types) == 3\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name\n",
    "            if name == \"West Lexington St.\":\n",
    "                assert better_name == \"West Lexington Street\"\n",
    "            if name == \"Baldwin Rd.\":\n",
    "                assert better_name == \"Baldwin Road\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QUIZ 6: Preparing for database\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\"\"\"\n",
    "Your task is to wrangle the data and transform the shape of the data\n",
    "into the model we mentioned earlier. The output should be a list of dictionaries\n",
    "that look like this:\n",
    "\n",
    "{\n",
    "\"id\": \"2406124091\",\n",
    "\"type: \"node\",\n",
    "\"visible\":\"true\",\n",
    "\"created\": {\n",
    "          \"version\":\"2\",\n",
    "          \"changeset\":\"17206049\",\n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
    "          \"user\":\"linuxUser16\",\n",
    "          \"uid\":\"1219059\"\n",
    "        },\n",
    "\"pos\": [41.9757030, -87.6921867],\n",
    "\"address\": {\n",
    "          \"housenumber\": \"5157\",\n",
    "          \"postcode\": \"60625\",\n",
    "          \"street\": \"North Lincoln Ave\"\n",
    "        },\n",
    "\"amenity\": \"restaurant\",\n",
    "\"cuisine\": \"mexican\",\n",
    "\"name\": \"La Cabana De Don Luis\",\n",
    "\"phone\": \"1 (773)-271-5176\"\n",
    "}\n",
    "\n",
    "You have to complete the function 'shape_element'.\n",
    "We have provided a function that will parse the map file, and call the function with the element\n",
    "as an argument. You should return a dictionary, containing the shaped data for that element.\n",
    "We have also provided a way to save the data in a file, so that you could use\n",
    "mongoimport later on to import the shaped data into MongoDB. \n",
    "\n",
    "Note that in this exercise we do not use the 'update street name' procedures\n",
    "you worked on in the previous exercise. If you are using this code in your final\n",
    "project, you are strongly encouraged to use the code from previous exercise to \n",
    "update the street names before you save them to JSON. \n",
    "\n",
    "In particular the following things should be done:\n",
    "- you should process only 2 types of top level tags: \"node\" and \"way\"\n",
    "- all attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:\n",
    "    - attributes in the CREATED array should be added under a key \"created\"\n",
    "    - attributes for latitude and longitude should be added to a \"pos\" array,\n",
    "      for use in geospacial indexing. Make sure the values inside \"pos\" array are floats\n",
    "      and not strings. \n",
    "- if the second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "- if the second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "- if the second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can\n",
    "  process it in a way that you feel is best. For example, you might split it into a two-level\n",
    "  dictionary like with \"addr:\", or otherwise convert the \":\" to create a valid key.\n",
    "- if there is a second \":\" that separates the type/direction of a street,\n",
    "  the tag should be ignored, for example:\n",
    "\n",
    "<tag k=\"addr:housenumber\" v=\"5158\"/>\n",
    "<tag k=\"addr:street\" v=\"North Lincoln Avenue\"/>\n",
    "<tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "<tag k=\"addr:street:prefix\" v=\"North\"/>\n",
    "<tag k=\"addr:street:type\" v=\"Avenue\"/>\n",
    "<tag k=\"amenity\" v=\"pharmacy\"/>\n",
    "\n",
    "  should be turned into:\n",
    "\n",
    "{...\n",
    "\"address\": {\n",
    "    \"housenumber\": 5158,\n",
    "    \"street\": \"North Lincoln Avenue\"\n",
    "}\n",
    "\"amenity\": \"pharmacy\",\n",
    "...\n",
    "}\n",
    "\n",
    "- for \"way\" specifically:\n",
    "\n",
    "  <nd ref=\"305896090\"/>\n",
    "  <nd ref=\"1719825889\"/>\n",
    "\n",
    "should be turned into\n",
    "\"node_refs\": [\"305896090\", \"1719825889\"]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node['id'] = element.attrib['id']\n",
    "        node['type'] = element.tag\n",
    "        node['created'] = {k: element.attrib[k] for k in CREATED}\n",
    "        if 'visible' in element.attrib:\n",
    "            node['visible'] = element.attrib['visible']\n",
    "        if element.tag != \"way\":\n",
    "            node['pos'] = [float(element.attrib['lat']), float(element.attrib['lon'])]\n",
    "        node['address'] = {}\n",
    "        for tag in element.iter('tag'):\n",
    "            attribute_string =  tag.attrib['k'].split(':')\n",
    "            node[attribute_string[0]] = {}\n",
    "            length = len(attribute_string)\n",
    "            if length > 2:\n",
    "                pass\n",
    "            elif length == 2:\n",
    "                if attribute_string[0] == 'addr':\n",
    "                    node['address'].update({attribute_string[1]:tag.attrib['v']})\n",
    "                else:\n",
    "                    node[attribute_string[0]].update({attribute_string[1]:tag.attrib['v']})\n",
    "            else:\n",
    "                node[attribute_string[0]] = tag.attrib['v']\n",
    "        \n",
    "        if element.tag == 'way':\n",
    "            node[\"node_refs\"] = []\n",
    "            for tag in element.iter('nd'):\n",
    "                node['node_refs'].append(tag.attrib['ref'])\n",
    "        node = {k: v for k, v in node.items() if v}\n",
    "        \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "def test():\n",
    "    # NOTE: if you are running this code on your computer, with a larger dataset, \n",
    "    # call the process_map procedure with pretty=False. The pretty=True option adds \n",
    "    # additional spaces to the output, making it significantly larger.\n",
    "    data = process_map('example.osm', True)\n",
    "    #pprint.pprint(data)\n",
    "    \n",
    "    correct_first_elem = {\n",
    "        \"id\": \"261114295\", \n",
    "        \"visible\": \"true\", \n",
    "        \"type\": \"node\", \n",
    "        \"pos\": [41.9730791, -87.6866303], \n",
    "        \"created\": {\n",
    "            \"changeset\": \"11129782\", \n",
    "            \"user\": \"bbmiller\", \n",
    "            \"version\": \"7\", \n",
    "            \"uid\": \"451048\", \n",
    "            \"timestamp\": \"2012-03-28T18:31:23Z\"\n",
    "        }\n",
    "    }\n",
    "    assert data[0] == correct_first_elem\n",
    "    assert data[-1][\"address\"] == {\n",
    "                                    \"street\": \"West Lexington St.\", \n",
    "                                    \"housenumber\": \"1412\"\n",
    "                                      }\n",
    "    assert data[-1][\"node_refs\"] == [ \"2199822281\", \"2199822390\",  \"2199822392\", \"2199822369\", \n",
    "                                    \"2199822370\", \"2199822284\", \"2199822281\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
